{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import & Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01QAMlQXEpbB"
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "from enum import Enum\n",
    "import math\n",
    "import torch.ao.quantization as tq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.quantized as nnq\n",
    "import pyverilator                     # pip install pyverilator\n",
    "\n",
    "# 參數\n",
    "num_classes = 100\n",
    "input_shape = (32, 32, 3)\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "image_size = 72\n",
    "patch_size = 6\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [projection_dim * 2, projection_dim]\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]\n",
    "\n",
    "MODEL_PATH = \"./weights/vit.pt\"\n",
    "QUANT_MODEL_PATH = \"./weights/quant_vit.pt\"\n",
    "\n",
    "DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Is cuda avaliable: \" , torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1ZWFs8cE6BN",
    "outputId": "28d1577c-53e3-45e6-9b64-b5b333d6c608"
   },
   "outputs": [],
   "source": [
    "def get_cifar_loaders(batch_size, root=\"data/cifar100\", split_ratio=0.1, image_size=72):\n",
    "    # train 資料增強\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(2),\n",
    "        transforms.RandomAffine(0, translate=(0.2, 0.2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408),  # CIFAR-100 平均值\n",
    "                             (0.2675, 0.2565, 0.2761)), # CIFAR-100 標準差\n",
    "    ])\n",
    "    # val/test 無增強\n",
    "    eval_transform = transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4867, 0.4408),\n",
    "                             (0.2675, 0.2565, 0.2761)),\n",
    "    ])\n",
    "    trainset = datasets.CIFAR10(root=root, train=True,  download=True, transform=transform)\n",
    "    testset  = datasets.CIFAR10(root=root, train=False, download=True, transform=eval_transform)\n",
    "    # 切分出小比例當 val\n",
    "    val_len   = int(split_ratio * len(trainset))\n",
    "    train_len = len(trainset) - val_len\n",
    "    trainset, valset = random_split(trainset, [train_len, val_len])\n",
    "    return (\n",
    "        DataLoader(trainset, batch_size=batch_size, shuffle=True,  num_workers=2),\n",
    "        DataLoader(valset,   batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        DataLoader(testset,  batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wjZEfQgKFCKQ"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for units in hidden_units:\n",
    "            layers.append(nn.Linear(units[0], units[1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mlp_params(pt_path,\n",
    "                       w1_key=\"mlp.0.weight\", b1_key=\"mlp.0.bias\",\n",
    "                       w2_key=\"mlp.3.weight\", b2_key=\"mlp.3.bias\"):\n",
    "    \"\"\"\n",
    "    從 .pt 檔撈兩層 Linear 的權重/偏差，回傳 tuple:\n",
    "    (w1, b1, w2, b2)，皆為 numpy array (float32)\n",
    "    \"\"\"\n",
    "    sd = torch.load(pt_path, map_location=\"cpu\")        # state_dict 或整個 nn.Module\n",
    "    if isinstance(sd, torch.nn.Module):\n",
    "        sd = sd.state_dict()\n",
    "\n",
    "    w1 = sd[w1_key].cpu().numpy().astype(np.float32)    # shape [out1, in1]\n",
    "    b1 = sd[b1_key].cpu().numpy().astype(np.float32)    # shape [out1]\n",
    "    w2 = sd[w2_key].cpu().numpy().astype(np.float32)    # shape [out2, in2]\n",
    "    b2 = sd[b2_key].cpu().numpy().astype(np.float32)    # shape [out2]\n",
    "\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "def quantize_w_int8(W):\n",
    "    s = np.max(np.abs(W))\n",
    "    scale = s / 127.0 if s != 0 else 1.0\n",
    "    q = np.round(W / scale).astype(np.int8)\n",
    "    return q, scale\n",
    "\n",
    "def quantize_b_int32(b, in_scale, w_scale):\n",
    "    scale = in_scale * w_scale\n",
    "    q = np.round(b / scale).astype(np.int32)\n",
    "    return q\n",
    "\n",
    "def pack_int8_to_words(arr_int8):\n",
    "    \"\"\"\n",
    "    arr_int8: 1D np.int8\n",
    "    回傳 List[int32]，每 4 個 byte 拼 1 word, 低位在前 (little-endian)。\n",
    "    \"\"\"\n",
    "    assert arr_int8.ndim == 1 and len(arr_int8) % 4 == 0\n",
    "    words = []\n",
    "    it = arr_int8.astype(np.uint8)        # 先轉成 0~255\n",
    "    for i in range(0, len(it), 4):\n",
    "        w =  int(it[i+0])        \\\n",
    "          | (int(it[i+1]) <<  8) \\\n",
    "          | (int(it[i+2]) << 16) \\\n",
    "          | (int(it[i+3]) << 24)\n",
    "        words.append(w)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 硬體MLP模擬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- hardware_mlp.py ----------------------------\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pyverilator                       # pip install pyverilator\n",
    "\n",
    "# -------- 量化工具 ------------------------------------------------------------\n",
    "def quantize_w_int8(w: np.ndarray):\n",
    "    s = np.max(np.abs(w))\n",
    "    scale = s / 127. if s != 0 else 1.\n",
    "    return np.round(w / scale).astype(np.int8), scale\n",
    "\n",
    "def quantize_b_int32(b: np.ndarray, in_scale: float, w_scale: float):\n",
    "    scale = in_scale * w_scale\n",
    "    return np.round(b / scale).astype(np.int32)\n",
    "\n",
    "def pack_int8_to_words(vec: np.ndarray):\n",
    "    \"\"\"每 4 個 int8 → 1 個 little-endian int32\"\"\"\n",
    "    assert vec.ndim == 1 and len(vec) % 4 == 0\n",
    "    words = []\n",
    "    u = vec.astype(np.uint8)\n",
    "    for i in range(0, len(u), 4):\n",
    "        words.append(int(u[i]) | int(u[i+1]) << 8 |\n",
    "                     int(u[i+2]) << 16 | int(u[i+3]) << 24)\n",
    "    return words\n",
    "\n",
    "# -------- 權重/偏差抽取 --------------------------------------------------------\n",
    "def extract_mlp_params(pt_path, keys):\n",
    "    \"\"\"keys = (w1, b1, w2, b2)\"\"\"\n",
    "    sd = torch.load(pt_path, map_location=\"cpu\")\n",
    "    if isinstance(sd, torch.nn.Module):\n",
    "        sd = sd.state_dict()\n",
    "\n",
    "    w1 = sd[keys[0]].cpu().numpy().astype(np.float32)\n",
    "    b1 = sd[keys[1]].cpu().numpy().astype(np.float32)\n",
    "    w2 = sd[keys[2]].cpu().numpy().astype(np.float32)\n",
    "    b2 = sd[keys[3]].cpu().numpy().astype(np.float32)\n",
    "    return w1, b1, w2, b2\n",
    "\n",
    "# -------- PyVerilator Wrapper  -------------------------------------------------\n",
    "class HardwareMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer int8 MLP implemented in Top.sv.\n",
    "    * feature_dim 必須和 ViT projection_dim 相同 (預設 64)\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 top_rtl=\"Top.sv\",\n",
    "                 feature_dim=64,\n",
    "                 input_scale=0.008,      # 根據校準結果填\n",
    "                 output_scale=0.008,\n",
    "                 scaling_factor=32):     # Top.sv PPU 右移 5bit (= 32)\n",
    "        super().__init__()\n",
    "        self.feature_dim   = feature_dim\n",
    "        self.input_scale   = input_scale\n",
    "        self.output_scale  = output_scale\n",
    "        self.scaling_factor = scaling_factor\n",
    "\n",
    "        self.sim = pyverilator.PyVerilator.build(top_rtl, trace_en=False)\n",
    "        self._hw_reset()\n",
    "\n",
    "    # ---------------- low-level ----------------\n",
    "    def _tick(self, n=1):\n",
    "        for _ in range(n):\n",
    "            self.sim.clock.tick()\n",
    "\n",
    "    def _hw_reset(self):\n",
    "        self.sim.io.rst = 1; self._tick(); self.sim.io.rst = 0\n",
    "\n",
    "    def _write_stream_to_sram(self, mode: int, stream):\n",
    "        \"\"\"依 tb.sv handshake：ready↑→送 data_in+i_en\"\"\"\n",
    "        self.sim.io.mode  = mode\n",
    "        self.sim.io.ready = 1\n",
    "        self._tick(); self.sim.io.ready = 0\n",
    "        for wd in stream:\n",
    "            self.sim.io.data_in = wd\n",
    "            self.sim.io.i_en    = 1\n",
    "            self._tick()\n",
    "        self.sim.io.i_en = 0\n",
    "\n",
    "    # ---------------- Public API ----------------\n",
    "    def load_from_pt(self, pt_path, pt_keys):\n",
    "        \"\"\"\n",
    "        1. 從 .pt 擷取 Linear1/2 權重 2. 量化 3. 寫入 SRAM\n",
    "        pt_keys = (w1, b1, w2, b2)\n",
    "        \"\"\"\n",
    "        w1, b1, w2, b2 = extract_mlp_params(pt_path, pt_keys)\n",
    "\n",
    "        w1_q, s_w1 = quantize_w_int8(w1)\n",
    "        w2_q, s_w2 = quantize_w_int8(w2)\n",
    "        b1_q = quantize_b_int32(b1, self.input_scale, s_w1)\n",
    "        b2_q = quantize_b_int32(b2, s_w1,          s_w2)\n",
    "\n",
    "        # 打包成 32-bit stream\n",
    "        w1_stream = pack_int8_to_words(w1_q.flatten())\n",
    "        w2_stream = pack_int8_to_words(w2_q.flatten())\n",
    "        b1_stream = [int(v & 0xFFFFFFFF) for v in b1_q]\n",
    "        b2_stream = [int(v & 0xFFFFFFFF) for v in b2_q]\n",
    "\n",
    "        # 依 RTL mode 寫入：2=W1 3=B1 4=W2 5=B2\n",
    "        self._hw_reset()\n",
    "        self._write_stream_to_sram(2, w1_stream)\n",
    "        self._write_stream_to_sram(3, b1_stream)\n",
    "        self._write_stream_to_sram(4, w2_stream)\n",
    "        self._write_stream_to_sram(5, b2_stream)\n",
    "        self.sim.io.mode = 0\n",
    "\n",
    "    # ---------------- forward ----------------\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x : [batch, feature_dim] float32\n",
    "        回傳同形狀 float32 tensor\n",
    "        \"\"\"\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "        batch = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        for i in range(batch):\n",
    "            vec = x[i]\n",
    "            # --- quantize ---\n",
    "            xi8 = np.round(vec.numpy() / self.input_scale) \\\n",
    "                                                       .clip(-128, 127).astype(np.int8)\n",
    "            # --- handshake: ready↑ (1clk) → 送 ifmap 16 words → wait done ---\n",
    "            self.sim.io.mode           = 0\n",
    "            self.sim.io.scaling_factor = self.scaling_factor\n",
    "            self.sim.io.ready = 1; self._tick(); self.sim.io.ready = 0\n",
    "\n",
    "            for j in range(0, self.feature_dim, 4):\n",
    "                wd = (int(xi8[j  ]) & 0xFF)       | (int(xi8[j+1]) & 0xFF) << 8 | \\\n",
    "                     (int(xi8[j+2]) & 0xFF) << 16 | (int(xi8[j+3]) & 0xFF) << 24\n",
    "                self.sim.io.data_in = wd\n",
    "                self._tick()\n",
    "\n",
    "            yo = []\n",
    "            while len(yo) < self.feature_dim:\n",
    "                self._tick()\n",
    "                if int(self.sim.io.valid):\n",
    "                    yo.append(self.sim.io.ofmap & 0xFF)\n",
    "            y_f = torch.tensor(yo, dtype=torch.float32) * self.output_scale\n",
    "            outs.append(y_f)\n",
    "        return torch.stack(outs, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "sOI2MMKpE-cO"
   },
   "outputs": [],
   "source": [
    "class Patches(nn.Module):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def forward(self, images):\n",
    "        # images: [B, C, H, W]\n",
    "        B, C, H, W = images.shape\n",
    "        ph, pw = self.patch_size, self.patch_size\n",
    "        assert H % ph == 0 and W % pw == 0\n",
    "        # 轉成 [B, num_patches, patch_dim]\n",
    "        patches = images.unfold(2, ph, ph).unfold(3, pw, pw)  # [B, C, nph, npw, ph, pw]\n",
    "        patches = patches.permute(0, 2, 3, 1, 4, 5)  # [B, nph, npw, C, ph, pw]\n",
    "        patches = patches.reshape(B, -1, C * ph * pw)  # [B, num_patches, patch_dim]\n",
    "        return patches\n",
    "\n",
    "class PatchEncoder(nn.Module):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(patch_size * patch_size * 3, projection_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, projection_dim))\n",
    "        self.adder   = nnq.FloatFunctional()\n",
    "        self.quant   = tq.QuantStub()\n",
    "        self.dequant = tq.DeQuantStub()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 1) 如果是量化張量，反量化到 float\n",
    "        if x.is_quantized:\n",
    "            x = self.dequant(x)\n",
    "        # 2) 在 float 做投影 + 位置加法\n",
    "        x = self.proj(x) + self.pos_embed\n",
    "        # 3) 再一次量化\n",
    "        return self.quant(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H_0sl4u6FFHT"
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, projection_dim, num_heads, transformer_units, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(projection_dim)\n",
    "        self.attn = nn.MultiheadAttention(projection_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(projection_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(projection_dim, transformer_units[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(transformer_units[0], transformer_units[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, num_patches, projection_dim]\n",
    "        x1 = self.norm1(x)\n",
    "        attn_output, _ = self.attn(x1, x1, x1)\n",
    "        x2 = attn_output + x\n",
    "        x3 = self.norm2(x2)\n",
    "        x3 = self.mlp(x3)\n",
    "        return x3 + x2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "3eiyjYrPFLcy"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, projection_dim, num_patches, num_heads, transformer_units, transformer_layers, mlp_head_units):\n",
    "        super().__init__()\n",
    "        self.patches = Patches(patch_size)\n",
    "        self.encoder = PatchEncoder(num_patches, projection_dim)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerBlock(projection_dim, num_heads, transformer_units)\n",
    "            for _ in range(transformer_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(projection_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        mlp_layers = []\n",
    "        in_dim = num_patches * projection_dim\n",
    "        for out_dim in mlp_head_units:\n",
    "            mlp_layers.append(nn.Linear(in_dim, out_dim))\n",
    "            mlp_layers.append(nn.GELU())\n",
    "            mlp_layers.append(nn.Dropout(0.5))\n",
    "            in_dim = out_dim\n",
    "        self.mlp_head = nn.Sequential(*mlp_layers)\n",
    "        self.classifier = nn.Linear(in_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, 3, H, W]\n",
    "        x = self.patches(x)  # [B, num_patches, patch_dim]\n",
    "        x = self.encoder(x)  # [B, num_patches, projection_dim]\n",
    "        for block in self.transformer_layers:\n",
    "            x = block(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mlp_head(x)\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device=DEFAULT_DEVICE):\n",
    "    model.eval()\n",
    "    running_loss, total, correct = 0.0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images) if device == \"cuda\" else model(images.cpu())\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = running_loss / len(loader)\n",
    "    acc = correct / total\n",
    "    cm  = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, acc, cm\n",
    "    \n",
    "def plot_loss_accuracy(\n",
    "    train_loss, val_loss,\n",
    "    train_top1, val_top1,\n",
    "    train_top5, val_top5,\n",
    "    filename=\"loss_accuracy.png\"\n",
    "):\n",
    "    # 建立 1×3 的子圖：Loss / Top-1 / Top-5\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    # 1) Loss\n",
    "    ax1.plot(train_loss, label=\"Train\", color=\"C0\")\n",
    "    ax1.plot(val_loss,   label=\"Val\",   color=\"C1\")\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # 2) Top-1 Accuracy\n",
    "    ax2.plot(train_top1, label=\"Train Top-1\", color=\"C0\")\n",
    "    ax2.plot(val_top1,   label=\"Val Top-1\",   color=\"C1\")\n",
    "    ax2.set_title(\"Top-1 Accuracy\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # 3) Top-5 Accuracy\n",
    "    ax3.plot(train_top5, label=\"Train Top-5\", color=\"C0\")\n",
    "    ax3.plot(val_top5,   label=\"Val Top-5\",   color=\"C1\")\n",
    "    ax3.set_title(\"Top-5 Accuracy\")\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Accuracy\")\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # 儲存與顯示\n",
    "    os.makedirs(os.path.dirname(filename) or \".\", exist_ok=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "def preprocess_filename(filename: str = MODEL_PATH, existed: str = \"keep_both\") -> str:\n",
    "    if existed == \"overwrite\":\n",
    "        pass\n",
    "    elif existed == \"keep_both\":\n",
    "        base, ext = os.path.splitext(filename)\n",
    "        cnt = 1\n",
    "        while os.path.exists(filename):\n",
    "            filename = f\"{base}-{cnt}{ext}\"\n",
    "            cnt += 1\n",
    "    elif existed == \"raise\" and os.path.exists(filename):\n",
    "        raise FileExistsError(f\"{filename} already exists.\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value for 'existed': {existed}\")\n",
    "    return filename\n",
    "    \n",
    "def save_model(\n",
    "    model, filename: str = MODEL_PATH, verbose: bool = True, existed: str = \"keep_both\"\n",
    ") -> None:\n",
    "    filename = preprocess_filename(filename, existed)\n",
    "\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    if verbose:\n",
    "        print(f\"Model saved at {filename} ({os.path.getsize(filename) / 1e6} MB)\")\n",
    "    else:\n",
    "        print(f\"Model saved at {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 744
    },
    "id": "_TR_w6QxFUaD",
    "outputId": "4baf07f8-adee-4c8b-ee4e-222b6ba19b96"
   },
   "outputs": [],
   "source": [
    "device = torch.device(DEFAULT_DEVICE)\n",
    "model = ViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    projection_dim=projection_dim,\n",
    "    num_patches=num_patches,\n",
    "    num_heads=num_heads,\n",
    "    transformer_units=[projection_dim*2, projection_dim],\n",
    "    transformer_layers=transformer_layers,\n",
    "    mlp_head_units=mlp_head_units\n",
    ").to(device)\n",
    "trainloader, valloader, testloader = get_cifar_loaders(batch_size, image_size=image_size)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loss, train_acc1 = [], []\n",
    "train_acc5 = []\n",
    "val_loss, val_acc1 = [], []\n",
    "val_acc5 = []\n",
    "\n",
    "t0 = time.time()\n",
    "best_epoch_train_acc = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total, correct_1, correct_5, val_1, val_5 = 0, 0, 0, 0, 0\n",
    "\n",
    "    loop = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "    for images, labels in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, preds_1 = logits.topk(1, dim=1)  \n",
    "        _, preds_5 = logits.topk(5, dim=1)  \n",
    "        total += labels.size(0)\n",
    "        correct_1 += (preds_1.squeeze(1) == labels).sum().item()\n",
    "        correct_5 += (preds_5 == labels.view(-1,1)).any(dim=1).sum().item()\n",
    "        # 動態顯示當前 loss 和 accuracy\n",
    "        loop.set_postfix(loss=running_loss/(total/images.shape[0]), top1_acc=correct_1/total, top5_acc=correct_5/total)\n",
    "        \n",
    "    epoch_train_loss = running_loss / len(trainloader)\n",
    "    epoch_train_acc = correct_1 / total\n",
    "    train_loss.append(epoch_train_loss)\n",
    "    train_acc1.append(epoch_train_acc)\n",
    "    train_acc5.append(correct_5 / total)\n",
    "\n",
    "    # 每次訓練完做一次validation\n",
    "    with torch.no_grad():\n",
    "        val_running_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        loop2 = tqdm(valloader, desc=\"Validation\", leave=True)\n",
    "        for images, labels in loop2:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_running_loss += loss.item()\n",
    "            _, preds_1 = logits.topk(1, dim=1)  \n",
    "            _, preds_5 = logits.topk(5, dim=1)  \n",
    "            val_total += labels.size(0)\n",
    "            val_1 += (preds_1.squeeze(1) == labels).sum().item()\n",
    "            val_5 += (preds_5 == labels.view(-1,1)).any(dim=1).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_running_loss / len(valloader)\n",
    "        epoch_val_acc = val_1 / val_total\n",
    "        val_loss.append(epoch_val_loss)\n",
    "        val_acc1.append(epoch_val_acc)\n",
    "        val_acc5.append(val_5 / val_total)\n",
    "    # save model if better\n",
    "    if epoch_train_acc > best_epoch_train_acc:\n",
    "        best_epoch_train_acc = epoch_train_acc\n",
    "        save_model(model, MODEL_PATH, existed=\"overwrite\")\n",
    "\n",
    "    \n",
    "# 測試\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_running_loss, correct_1, correct_5, val_total = 0.0, 0, 0, 0\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, labels)\n",
    "        val_running_loss += loss.item()\n",
    "        _, preds_1 = logits.topk(1, dim=1)  \n",
    "        _, preds_5 = logits.topk(5, dim=1)  \n",
    "        val_total += labels.size(0)\n",
    "        correct_1 += (preds_1.squeeze(1) == labels).sum().item()\n",
    "        correct_5 += (preds_5 == labels.view(-1,1)).any(dim=1).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / len(valloader)\n",
    "    epoch_val_acc_1 = correct_1 / val_total\n",
    "    epoch_val_acc_5 = correct_5 / val_total\n",
    "\n",
    "# 5. 繪製並儲存訓練曲線\n",
    "os.makedirs(\"figure\", exist_ok=True)\n",
    "\n",
    "plot_loss_accuracy(\n",
    "    train_loss=train_loss, val_loss=val_loss,\n",
    "    train_top1=train_acc1, val_top1=val_acc1,\n",
    "    train_top5=train_acc1, val_top5=val_acc5,\n",
    "    filename=\"figure/vit_loss_accuracy.png\"\n",
    ");\n",
    "# 6. 測試並顯示最終結果\n",
    "#test_loss, test_accuracy, _ = evaluate(model, testloader, criterion)\n",
    "print(f\"Test loss={epoch_val_loss:.4f}, top1 accuracy={epoch_val_acc_1:.4f}, top1 accuracy={epoch_val_acc_5:.4f}\")\n",
    "print(f\"Total training time: {time.time() - t0:.1f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT + 硬體 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------- vit_hw.py -----------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from vit_original import ViT, TransformerBlock   # 假設兩個 class 在原 ViT.py\n",
    "\n",
    "# ---- 修改 TransformerBlock：加可插拔 mlp ----------------\n",
    "class TransformerBlockHW(TransformerBlock):\n",
    "    def __init__(self, projection_dim, num_heads, transformer_units,\n",
    "                 dropout=0.1, hw_mlp=None):\n",
    "        super().__init__(projection_dim, num_heads, transformer_units, dropout)\n",
    "        if hw_mlp is not None:\n",
    "            self.mlp = hw_mlp  # 把 nn.Sequential 換掉\n",
    "\n",
    "# ---- ViT 包裝：在 __init__ 傳進同一顆 hw_mlp ----\n",
    "class ViT_Hardware(ViT):\n",
    "    def __init__(self, hw_mlp=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if hw_mlp is not None:\n",
    "            # 用新的 block 取代\n",
    "            self.transformer_layers = nn.ModuleList([\n",
    "                TransformerBlockHW(kwargs['projection_dim'],\n",
    "                                   kwargs['num_heads'],\n",
    "                                   kwargs['transformer_units'],\n",
    "                                   hw_mlp=hw_mlp)\n",
    "                for _ in range(kwargs['transformer_layers'])\n",
    "            ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 硬體測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- train_and_eval.py --------------------------\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from hardware_mlp import HardwareMLP\n",
    "from vit_hw import ViT_Hardware\n",
    "\n",
    "# ---- Dummy dataset -----------------------------------------------------------\n",
    "def make_dummy_loader(batch=4, n_batch=20):\n",
    "    x = torch.rand(batch * n_batch, 3, 224, 224)\n",
    "    y = torch.randint(0, 10, (batch * n_batch,))\n",
    "    ds = TensorDataset(x, y)\n",
    "    return DataLoader(ds, batch_size=batch)\n",
    "\n",
    "# ---- 1. 建 ViT (訓練階段先用軟體 MLP) ---------------------------------------\n",
    "model = ViT_Hardware(           # hw_mlp=None → 全軟體\n",
    "    hw_mlp=None,\n",
    "    image_size=224, patch_size=16, num_classes=10,\n",
    "    projection_dim=64, num_patches=(224//16)**2,\n",
    "    num_heads=4, transformer_units=[128, 64],\n",
    "    transformer_layers=1,       # demo 用 1 層\n",
    "    mlp_head_units=[128]\n",
    ").train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loader = make_dummy_loader()\n",
    "\n",
    "# ---- 2. 簡易 train 1 epoch ----------------------------------------------------\n",
    "for xb, yb in loader:\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(xb)\n",
    "    loss = criterion(logits, yb)\n",
    "    loss.backward(); optimizer.step()\n",
    "print(\"train done, loss =\", loss.item())\n",
    "\n",
    "torch.save(model.state_dict(), \"latest.pt\")\n",
    "\n",
    "# ---- 3. 構建硬體 MLP + 灌入剛存的權重 ----------------------------------------\n",
    "hw_mlp = HardwareMLP(feature_dim=64,\n",
    "                     input_scale=0.008, output_scale=0.008,\n",
    "                     scaling_factor=32)\n",
    "hw_mlp.load_from_pt(\n",
    "    \"latest.pt\",\n",
    "    pt_keys=(\"transformer_layers.0.mlp.0.weight\",\n",
    "             \"transformer_layers.0.mlp.0.bias\",\n",
    "             \"transformer_layers.0.mlp.3.weight\",\n",
    "             \"transformer_layers.0.mlp.3.bias\")\n",
    ")\n",
    "\n",
    "# ---- 4. 重新建立 “部署版” ViT：MLP → 硬體 -----------------------------------\n",
    "vit_deploy = ViT_Hardware(\n",
    "    hw_mlp=hw_mlp,\n",
    "    image_size=224, patch_size=16, num_classes=10,\n",
    "    projection_dim=64, num_patches=(224//16)**2,\n",
    "    num_heads=4, transformer_units=[128, 64],\n",
    "    transformer_layers=1, mlp_head_units=[128]\n",
    ").eval()\n",
    "\n",
    "# ---- 5. 驗證硬體輸出 vs 軟體輸出 --------------------------------------------\n",
    "x_test = torch.randn(2, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    y_hw = vit_deploy(x_test)             # MLP 經 Top.sv\n",
    "    # 為對比，build 另一個軟體-MLP 版本\n",
    "    model.eval()\n",
    "    y_sw = model(x_test)                  # 純 PyTorch\n",
    "\n",
    "diff = (y_hw - y_sw).abs().max().item()\n",
    "print(f\"Max |HW-SW| = {diff:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Y1F8P4u8H5wa"
   },
   "outputs": [],
   "source": [
    "class PowerOfTwoObserver(tq.MinMaxObserver):\n",
    "    \"\"\"\n",
    "    Observer module for power-of-two quantization (dyadic quantization with b = 1).\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def scale_approximate(self, scale: float, max_shift_amount=8) -> float:\n",
    "        #########Implement your code here##########\n",
    "        if scale <= 0:  # Handle non-positive scale values\n",
    "            return 2 ** -max_shift_amount  # Return a safe default value\n",
    "\n",
    "        # Calculate the power of 2 closest to the scale\n",
    "        exponent = math.log2(scale)\n",
    "\n",
    "        # Clamp the exponent to the allowed range\n",
    "        exponent = max(-max_shift_amount, min(exponent, max_shift_amount))\n",
    "\n",
    "        # Return the power of 2\n",
    "        return (2.0 ** exponent)\n",
    "        ##########################################\n",
    "\n",
    "    def calculate_qparams(self):\n",
    "        \"\"\"Calculates the quantization parameters with scale as power of two.\"\"\"\n",
    "        min_val, max_val = self.min_val.item(), self.max_val.item()\n",
    "\n",
    "        \"\"\" Calculate zero_point as in the base class \"\"\"\n",
    "        #########Implement your code here##########\n",
    "        max_num = 2**7 - 1\n",
    "        max_range = max(abs(min_val), abs(max_val))\n",
    "        scale = max_range / max_num\n",
    "        \n",
    "        zero_point = 0 if self.dtype == torch.qint8 else max_num\n",
    "        \n",
    "        ##########################################\n",
    "        scale = self.scale_approximate(scale)\n",
    "        scale = torch.tensor(scale, dtype=torch.float32)\n",
    "        zero_point = torch.tensor(zero_point, dtype=torch.int64)\n",
    "        return scale, zero_point\n",
    "\n",
    "class CustomQConfig(Enum):\n",
    "    POWER2 = tq.QConfig(\n",
    "        activation=PowerOfTwoObserver.with_args(\n",
    "            dtype=torch.quint8, qscheme=torch.per_tensor_symmetric\n",
    "        ),\n",
    "        weight=PowerOfTwoObserver.with_args(\n",
    "            dtype=torch.qint8, qscheme=torch.per_tensor_symmetric\n",
    "        ),\n",
    "    )\n",
    "    DEFAULT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''checkpoint = torch.load(MODEL_PATH, map_location='cpu')  # [2]\n",
    "\n",
    "# 如果你存的是 state_dict（dict of name→Tensor）\n",
    "if isinstance(checkpoint, dict):\n",
    "    print(\"Keys and tensor shapes in state_dict:\")\n",
    "    total_params = 0\n",
    "    for name, tensor in checkpoint.items():\n",
    "        shape = tuple(tensor.shape)\n",
    "        numel = tensor.numel()\n",
    "        total_params += numel\n",
    "        print(f\"  {name}: shape={shape}, params={numel}\")\n",
    "    print(f\"Total parameters: {total_params}\")\n",
    "else:\n",
    "    # 如果你存的是整個模型物件\n",
    "    print(\"Checkpoint is a model instance:\")\n",
    "    print(checkpoint)\n",
    "with open('model_parameters.txt', 'w') as f:\n",
    "    for name, tensor in checkpoint.items():\n",
    "        f.write(f\"{name}: {tuple(tensor.shape)}\\n\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pAEbbHPEH8xn",
    "outputId": "22de6afb-6fdf-4855-b448-00ef6e380fb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leoyo\\AppData\\Local\\Temp\\ipykernel_7836\\3436509919.py:31: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.prepare(model_q, inplace=True)\n",
      "Calibrating: 100%|███████████████████████████████████████████████████████████████████| 176/176 [01:01<00:00,  2.87it/s]\n",
      "C:\\Users\\leoyo\\AppData\\Local\\Temp\\ipykernel_7836\\3436509919.py:44: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  tq.convert(model_q, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model saved to ./weights/quant_vit.pt\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\qlinear.cpp:1436 [kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cudnn\\Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:375 [backend fallback]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocastMAIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:504 [backend fallback]\nAutocastXPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:542 [backend fallback]\nAutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:198 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 64\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m valloader:\n\u001b[0;32m     63\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(DEVICE), labels\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 64\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, labels\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[0;32m     66\u001b[0m     val_running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1767\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1776\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1780\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\ao\\quantization\\stubs.py:72\u001b[0m, in \u001b[0;36mQuantWrapper.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     71\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(X)\n\u001b[1;32m---> 72\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(X)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1767\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1776\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1780\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# x: [B, 3, H, W]\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatches(x)  \u001b[38;5;66;03m# [B, num_patches, patch_dim]\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [B, num_patches, projection_dim]\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_layers:\n\u001b[0;32m     28\u001b[0m         x \u001b[38;5;241m=\u001b[39m block(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1767\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1776\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1780\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[5], line 31\u001b[0m, in \u001b[0;36mPatchEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdequant(x)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 2) 在 float 做投影 + 位置加法\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# 3) 再一次量化\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1767\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\nn\\modules\\module.py:1778\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1776\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1778\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1780\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\linear.py:190\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_packed_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_point\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\aoc\\lib\\site-packages\\torch\\_ops.py:1208\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[1;32m-> 1208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'quantized::linear' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::linear' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cpu\\qlinear.cpp:1436 [kernel]\nQuantizedCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\quantized\\cudnn\\Linear.cpp:359 [kernel]\nBackendSelect: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:194 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:375 [backend fallback]\nNamed: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:466 [backend fallback]\nAutocastMAIA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:504 [backend fallback]\nAutocastXPU: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:542 [backend fallback]\nAutocastMPS: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:198 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# reset_seed(0)\n",
    "DEVICE = \"cpu\"\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "# 1. 先從 DataLoader 拿到一筆範例，讀出通道數與圖像大小\n",
    "in_channels, in_size = trainloader.dataset[0][0].shape[:2]\n",
    "\n",
    "# 2. 建立與訓練時完全相同的 ViT 架構，並切到 CPU、eval 模式\n",
    "model_q = ViT(\n",
    "    image_size=image_size,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    projection_dim=projection_dim,\n",
    "    num_patches=num_patches,\n",
    "    num_heads=num_heads,\n",
    "    transformer_units=transformer_units,\n",
    "    transformer_layers=transformer_layers,\n",
    "    mlp_head_units=mlp_head_units\n",
    ").eval().cpu()\n",
    "\n",
    "# 3. 載入你先前儲存的 state_dict\n",
    "model_q.load_state_dict(torch.load(MODEL_PATH, map_location=\"cpu\"))\n",
    "# --- 2. 包裝模型並指定 QConfig ---\n",
    "# ViT 是非常規 CNN，無 fuse 需求，直接包 QuantWrapper\n",
    "model_q = tq.QuantWrapper(model_q)  \n",
    "model_q.qconfig = CustomQConfig.POWER2.value\n",
    "# 放回CPU\n",
    "model_q.cpu()\n",
    "trainloader_cpu = DataLoader(trainloader.dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# --- 3. 準備量化 (插入 observer) ---\n",
    "tq.prepare(model_q, inplace=True)\n",
    "\n",
    "# --- 4. 校準 (用 trainloader 跑 forward) ---\n",
    "model_q.eval()\n",
    "with torch.no_grad():\n",
    "    #for i, (images, _) in enumerate(trainloader):\n",
    "    #    if i >= 10: break            # 取前 10 批做校準\n",
    "    #    model_q(images.to(DEVICE))\n",
    "    loop = tqdm(trainloader, desc=\"Calibrating\", leave=True)\n",
    "    for data, _ in loop:\n",
    "        data = data.to(DEVICE)\n",
    "        model_q(data.cpu())\n",
    "# --- 5. 轉換為量化模型 ---\n",
    "tq.convert(model_q, inplace=True)\n",
    "'''\n",
    "for name, module in model_q.named_modules():\n",
    "    if isinstance(module, nnq.Linear):\n",
    "        print(f\"量化成功層: {name}\")\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        print(f\"未量化層: {name}\")\n",
    "'''\n",
    "# --- 7. 儲存量化模型 ---\n",
    "os.makedirs(\"weights\", exist_ok=True)\n",
    "torch.save(model_q.state_dict(), QUANT_MODEL_PATH)\n",
    "print(\"Quantized model saved to \" + QUANT_MODEL_PATH)\n",
    "\n",
    "# --- 6. 評估量化後模型效能 ---\n",
    "#quant_loss, quant_acc, _ = evaluate(model_q, testloader, criterion, DEVICE)\n",
    "model_q.eval()\n",
    "with torch.no_grad():\n",
    "    val_running_loss, correct_1, correct_5, val_total = 0.0, 0, 0, 0\n",
    "    for images, labels in valloader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        logits = model_q(images.cpu())\n",
    "        loss = criterion(logits, labels.cpu())\n",
    "        val_running_loss += loss.item()\n",
    "        _, preds_1 = logits.topk(1, dim=1)  \n",
    "        _, preds_5 = logits.topk(5, dim=1)  \n",
    "        val_total += labels.size(0)\n",
    "        correct_1 += (preds_1.squeeze(1) == labels).sum().item()\n",
    "        correct_5 += (preds_5 == labels.view(-1,1)).any(dim=1).sum().item()\n",
    "\n",
    "    epoch_val_loss = val_running_loss / len(valloader)\n",
    "    epoch_val_acc_1 = correct_1 / val_total\n",
    "    epoch_val_acc_5 = correct_5 / val_total\n",
    "\n",
    "print(f\"Quantized top1 accurcay: {epoch_val_acc_1:.4f} Quantized top5 accurcay: {epoch_val_acc_5:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "#original_size = os.path.getsize('./weights/cifar10/vgg.pt') / 1e6\n",
    "#quantized_size = os.path.getsize(QUANT_MODEL_PATH) / 1e6\n",
    "#print(f\"Original model size: {original_size:.2f} MB\")\n",
    "#print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
    "#print(f\"Size reduction: {(1 - quantized_size/original_size) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "aoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
